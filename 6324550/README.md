# Pipeline de Produtos e Vendas - Exerc√≠cio Final

## üìã Perguntas e Respostas sobre a Implementa√ß√£o

### **Parte 1: An√°lise e Planejamento**

#### **Q1: Quais problemas foram identificados nos dados?**

**R:** Os dados apresentam os seguintes problemas:

- **Produtos:** Campo `Preco_Custo` nulo (P003) e `Fornecedor` nulo (P005)
- **Vendas:** Campo `Preco_Venda` nulo (V005)

#### **Q2: Qual estrat√©gia ETL foi definida e por qu√™?**

**R:** Foi escolhida a abordagem **ETL** porque:

- Dados estruturados e de pequeno volume
- Transforma√ß√µes simples (limpeza e c√°lculos)
- Melhor controle de qualidade antes do carregamento
- Menor uso de recursos no PostgreSQL de destino

### **Parte 2: Implementa√ß√£o das Tasks**

#### **Q3: Como foi implementada a Task `extract_produtos`?**

**R:** A task extrai dados do arquivo `produtos_loja.csv`:

```python
def extract_produtos():
    # L√™ CSV, valida exist√™ncia do arquivo
    # Retorna DataFrame via XCom
    # Log do n√∫mero de registros extra√≠dos
```

#### **Q4: Como foi implementada a Task `extract_vendas`?**

**R:** Similar √† extra√ß√£o de produtos, mas para `vendas_produtos.csv`:

```python
def extract_vendas():
    # L√™ CSV de vendas, valida arquivo
    # Retorna DataFrame via XCom
    # Log do n√∫mero de registros extra√≠dos
```

#### **Q5: Quais transforma√ß√µes foram aplicadas na Task `transform_data`?**

**R:** As seguintes transforma√ß√µes de limpeza e enriquecimento:

**Limpeza de dados:**

- `Preco_Custo` nulo ‚Üí preenchido com m√©dia da categoria
- `Fornecedor` nulo ‚Üí preenchido com "N√£o Informado"
- `Preco_Venda` nulo ‚Üí calculado como `Preco_Custo * 1.3`

**C√°lculos derivados:**

- `Receita_Total` = `Quantidade_Vendida * Preco_Venda`
- `Margem_Lucro` = `Preco_Venda - Preco_Custo`
- `Mes_Venda` = extra√≠do de `Data_Venda` (formato YYYY-MM)

#### **Q6: Como foi implementada a Task `create_tables`?**

**R:** Utiliza `PostgresOperator` para criar 4 tabelas:

- `produtos_processados` - dados de produtos limpos
- `vendas_processadas` - dados de vendas com c√°lculos
- `relatorio_vendas` - join consolidado
- `produtos_baixa_performance` - para o desafio b√¥nus

#### **Q7: Como funciona a Task `load_data`?**

**R:** Carrega os dados transformados no PostgreSQL:

- Recebe DataFrames via XCom das tasks anteriores
- Usa `PostgresHook` para conex√£o
- Insere dados nas tabelas com valida√ß√£o de sucesso
- Gera logs informativos do processo

#### **Q8: O que gera a Task `generate_report`?**

**R:** Produz relat√≥rio anal√≠tico com:

- Total de vendas por categoria
- Produto mais vendido
- Canal de venda com maior receita
- Margem de lucro m√©dia por categoria

### **Parte 3: Configura√ß√£o da DAG**

#### **Q9: Como foi configurada a DAG?**

**R:** Configura√ß√£o conforme especifica√ß√£o:

```python
dag_id='pipeline_produtos_vendas'
schedule='0 6 * * *'  # Di√°rio √†s 6h
retries=2
email_on_failure=False
tags=['produtos', 'vendas', 'exercicio', 'bonus']
```

#### **Q10: Como est√£o definidas as depend√™ncias entre tasks?**

**R:** Fluxo de depend√™ncias:

```
[extract_produtos, extract_vendas] >> transform_data
create_tables >> load_data
transform_data >> load_data
load_data >> [generate_report, analyze_performance]
```

### **Desafio B√¥nus**

#### **Q11: Como foi implementado o desafio b√¥nus?**

**R:** A task `analyze_performance` implementa:

**Funcionalidade:**

- Detecta produtos com menos de 2 vendas totais
- Usa CTE para agregar vendas por produto
- Gera alertas via log quando encontra baixa performance
- Carrega resultados na tabela `produtos_baixa_performance`

**Query SQL:**

```sql
WITH VendasAgregadas AS (
    SELECT nome_produto, categoria, SUM(quantidade_vendida) AS total_vendido
    FROM relatorio_vendas
    GROUP BY nome_produto, categoria
)
SELECT nome_produto, categoria, total_vendido,
       'BAIXA PERFORMANCE (< 2)' AS status_alerta
FROM VendasAgregadas
WHERE total_vendido < 2;
```

### **Estrutura do C√≥digo**

#### **Q12: Quais s√£o os componentes principais do arquivo?**

**R:** O arquivo cont√©m:

- **Imports:** Airflow, PostgreSQL, logging
- **Configura√ß√µes:** default_args, conex√µes, arquivos
- **Fun√ß√µes:** extract_produtos, extract_vendas, transform_data, load_data, generate_report, analyze_performance
- **DAG:** Defini√ß√£o e orquestra√ß√£o das tasks

#### **Q13: Como √© feito o tratamento de erros?**

**R:** Implementado atrav√©s de:

- Retry autom√°tico (2 tentativas)
- Delay entre tentativas (5 minutos)
- Logs informativos em cada etapa
- Valida√ß√µes de exist√™ncia de arquivos
- Verifica√ß√£o de dados carregados

### **Execu√ß√£o e Valida√ß√£o**

#### **Q14: Como executar e validar o pipeline?**

**R:** Passos para execu√ß√£o:

1. Colocar arquivos CSV na pasta `/opt/airflow/data/`
2. Ativar a DAG no Airflow UI
3. Executar manualmente ou aguardar schedule
4. Verificar logs de cada task
5. Validar dados no PostgreSQL:

```sql
SELECT COUNT(*) FROM produtos_processados;
SELECT COUNT(*) FROM vendas_processadas;
SELECT COUNT(*) FROM relatorio_vendas;
```

#### **Q15: Quais s√£o os crit√©rios de sucesso?**

**R:** Pipeline bem-sucedido quando:

- Todas as tasks executam sem erro
- Dados s√£o carregados nas 4 tabelas
- Relat√≥rio √© gerado com m√©tricas corretas
- Logs mostram informa√ß√µes detalhadas
- Desafio b√¥nus identifica produtos de baixa performance

---

## üöÄ Como Executar

1. **Preparar ambiente:**

   ```bash
   docker compose up -d
   ```

2. **Acessar Airflow:**

   - URL: http://localhost:8080
   - User: admin / Password: [airflow_token]

3. **Ativar DAG:**

   - Localizar `pipeline_produtos_vendas`
   - Ativar toggle
   - Executar manualmente

4. **Verificar resultados:**
   - Logs no Airflow UI
   - Dados no PostgreSQL (porta 2001)

## üìä Resultados Esperados

- **produtos_processados:** 5 registros limpos
- **vendas_processadas:** 5 registros com c√°lculos
- **relatorio_vendas:** 5 registros consolidados
- **produtos_baixa_performance:** Produtos com < 2 vendas
